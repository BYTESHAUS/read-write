# AI Twenties. Much ado about ... fussing?

In the heat records breaking mid-2020s _Artificial Intelligence_ is on trend again. Nobody will dodge this "IntelliSteamPunk" festival - __AI__ and __GPT__ are in every other lecture, article, or product release.

Torrents of news and announcements have urged prominent software engineers, scientists, and their leads to [think of a dam](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) against [AI&nbsp;dystopia](https://www.businessinsider.com/ai-extinction-risk-openai-deepmind-anthropic-ceos-sam-altman-2023-5), while other visions are:

<details><summary><ins>&nbsp;<b>Impressive mainstream</b>&nbsp;</ins></summary>
&nbsp;

> __AI is the most profound technology humanity is working on today.__\
*Sundar Pichai*, Google CEO

> **Software is eating the world, but AI is going to eat software.**\
*Jensen Huang*, CEO of NVIDIA

> **Generative AI’s impact on productivity could add trillions of dollars [annually]...**\
MkCinsey & Company

> **By 2029, computers will have emotional intelligence and be convincing as people.**\
*Ray Kurzweil*, inventor and futurist

> **There is no reason and no way that a human mind can keep up with an artificial intelligence machine by 2035.**\
_Gray Scott_, futurist, techno-philosopher, founder and CEO of SeriousWonder

\__________________________________________
</details>

<details><summary><ins>&nbsp;<b>Occasional pragmatism</b>&nbsp;</ins></summary>

* *Ginni Rometty*, former CEO of IBM:\
"Some people call this artificial intelligence, but the reality is this technology will enhance us. So instead of artificial intelligence, I think we'll augment our intelligence."

* *Larry Page*, Google co-founder:\
"Artificial intelligence would be the ultimate version of Google. ... Unfortunately, we’re nowhere near doing that now."\
[Well, it was in 2000]

* _Christopher Nolan_, filmmaker, questioned about AI in a 2023 interview:\
"... the real world is, by definition, infinitely complex. ... And so, any digital simulation or technology that simulates, eventually, it always hits a particular limitation."

* _Michael Atleson_, Attorney, Federal Trade Commission, 27/Feb/2023:\
"Keep your AI claims in check."

* _Sam Altman,_ [on/off] CEO of OpenAI, 04/Dec/2022:\
"**i am a stochastic parrot, and so r u**."&nbsp;<sup>🦜</sup>\
[vs. _Arikia Millikan_: [Parrots are not stochastic and neither are you](https://www.content-technologist.com/stochastic-parrots/).]

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>🦜</sup> <sub>The term Prof. Emily M. Bender _et al_ coined in her essay "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?".</sub>

\__________________________________________
</details>

<details><summary><ins>&nbsp;<b>Philistine margins</b>&nbsp;</ins></summary>
&nbsp;

<a href="../../memes#AIxorIT"><img src="../../../../_rsc/_img/memes/AI_medieval_now-spot_the_diff.jpg" alt="&nbsp;&nbsp;&nbsp;AI: 1523 vs. 2023. Spot the difference" title=" Answer: Homunculus shows the middle finger on the left hand." /></a><br />
*&nbsp;<sub>Images are for illustrative purposes only and were taken from Wiki Commons and IMLO (learncomputerscienceonline.com)</sub>

\__________________________________________
</details>

## Stumbling block

<table><tr valign="top"><td width="20%"><picture><img alt="&nbsp;Robot Nr5 from the movie Short Circuit, 1986" title="&nbsp;Tagline: Something wonderful has happened... Number Five is alive!"
     src="../../../../_rsc/_img/snap/1986.ShortCircuit-poster_Nr5.jpg" /></picture><br /><sub>"Short circuit", 1986</sub></td>
 <td width="60%"><div>Neither philosophy nor natural sciences can credibly formulate <i>intelligence</i>. Its genesis, individual rise, and (un)consciousness behind are a bigger mystery.&#8287;
<b>Digitalization speaks in precise definitions &mdash; and there are none.</b></div>
<blockquote><sub>Judging decision-making as a relay work of neurons preset by acquired knowledge is an ingenuous simplism of free will debate.</sub></blockquote>
  <div>Adepts of bright and heralds of dismal <samp><b>AI</b></samp> future are fain to assume its <i>self-knowledge/awareness</i> as a matter of time. &mdash; 
To brew by itself in increasingly complex code bouillon (heated by ultra-processors), or get all of a sudden <a href="https://www.imdb.com/title/tt0091949">short&nbsp;circuited</a><sup>🎦</sup>.</div>
</td>
 <td width="20%"><picture><img alt="René Descartes (1596-1610)" src="../../../../_rsc/_img/persona/ReneDescartes-portraint_by_Frans_Hals(frag).jpg" 
  title="&nbsp;Mathematician and philosopher René Descartes&#010;&nbsp;played a crucial role in presenting self-consciousness&#010;as the central philosophical concept." /></picture>
 <br /><sub>René Descartes (1596-1650)</sub>
</td></tr></table>

<b>IT</b> fantastically and reliably automates much of hard cogitative ops<sup>:coffee:</sup>, but not brittle abstraction, (self-)criticism, motivation - which are precursors of great enlightenings in sciences and arts<sup>:art:</sup>.\
&nbsp;&nbsp;&nbsp;&nbsp;<sup>:coffee:</sup> <sub>Where humans are slow, prone to errors, lack parallelism, and finally get distracted, fatigued, or frustrated.</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;<sup>:art:</sup> <sub>Fed with plain plots (keywords) bots may generate impressive video shorts, but it could be only a gifted artist who will weave a masterwork using this technique.</sub>

## Evidence

You still can think up a **🇨🇦**🅿️🪒🇨🇭🅰️, which a savvy person will untwist but Internet-connected AI-bot may only exhaust (unless you limit attempts)<sup>💫</sup>. The same fellow may create unique text, image, or script that GPT will pick in search results to quote.<sup>👓</sup>

The progress of computing powers allowed _assisted mathematical proofs_, which was impossible before. Artificial neural networks of all the processors haven't solved an open _mathematical problem_<sup>:1234:</sup>. Humans do.<sup>🧑‍🔬</sup>

It was a unique supercomputer (IBM Deep Blue) that finally beat the world chess champion in 1997 - now it could be amateur freeware on a smartphone to discard. However, humanoid developers wrote C/C++ for their engines (up to _Stockfish_ and _Leela Chess Zero_), while AI can't generate a better one. 

IDEs and assists may boost dev productivity, but if any AI claims to write the code, it will be either a predefined snippet or a rated Q&A or you need to describe the requirements closely to operators.

 _Supervised machine learning_, that nurtured the [Mechanical&nbsp;Turk](https://en.wikipedia.org/wiki/Mechanical_Turk)<sup><b>w</b></sup>, now shades significant human assistance and intervention backing many AI processes (individually by developers and by thousands in low-rate support). Many prolific contributors were stunned to know that their Q&A and imageboards supply chatbots with content under the agreement (they skipped to study).

👆 **Compare to hash functions on pure math, which had silently revolutionalized IT before the blockchain boom.**

\___________\
<sup>💫</sup> <sub>What's more significant humans may judge captcha as intentional or occasional nonsense and give up. And _vice versa_ - discern a challenge-response disguised within content or chaos.</sub>\
<sup>👓</sup> <sub>Vice versa insert of generated or pasted content in "original" publications is predetermined to disclosure.</sub>\
<sup>:1234:</sup> <sub>Conditions for many are plain and unambivalent enough to submit them as a clause to a chatbot.</sub>\
<sup>🧑‍🔬</sup> <sub>As <i>Grigori Perelman</i> with the sinlge solved of the seven <i>Millenium Problems</i> (the <i>Poincaré conjecture</i>, announced in 2010).</sub>

<table><tr><td width="40%"><picture><img alt="&nbsp;&nbsp;&nbsp;Sabotage of autonomous traffic in San Francisco, 2023" src="../../../../_rsc/_img/snap/media/2023.SF-Waymo_Sabotage(screengrab).jpg" title="&nbsp;Traffic cones block Waymo autonomous taxis in San-Francisco, 2023" /></picture></td><td>
<p>Driving is a vivid subject of automation, done almost to the whole extent, and <i>machine learning</i> must have closed the little remainder of less predictable hazards and blockages. </p>
<p>However after millions of miles in real traffic under the human safeguard no <i>deus ex machine</i> (aka <b>Full</b> Self-Driving) emerged. 
 Auto-pilot <i>level&nbsp;5</i> is a decade still "next year".</p>
</td></tr></table>

## Conclusions

 «AI» overhypes <samp><b>IT</b></samp> potential and credits groundbreaking products<sup>:hammer_and_wrench:</sup> with self-learning.
 
Protagonists put the synergism of natural intelligence<sup>🧑‍🔬</sup> on rising computing powers, and wrap it with HQ videos, splendid presentations, and compelling oration.

<samp>AI</samp> bots found their rich niche in recycling terabytes of rated input from millions of users. 
Famous brands apply "AI" to everything (like TV sets), and myriad startups seize the moment to sell sn**AI**ke oil for right everything: from a toothbrush to fortune-telling (to name decent).

&nbsp;&nbsp;&nbsp;&nbsp;<sup>:hammer_and_wrench:</sup> <sub>Processing global amounts of data in real-time, finding patterns there; on-the-fly video recognition and filtering; routine translation and text generation.</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;<sup>🧑‍🔬</sup> <sub>Math, physics, and linguistics R&D (as "neural" networks, quantum mechanics, stochastic models)</sub>x

## The Appendix. Preceding AI tides

Grunting "again"<sup>:top:</sup> prompts me to reminisce about other AI booms (impromptu and without any wiki). I skip precursors (Timber Intelligence of Pinocchio and a bit more scientific), the times when man humanized mechanical or electrical breakthroughs, and would resume on the digital era.

### The 1950s - the crucial IT decade ever - to the 1960s boom

Energized racks of metals, glass, and vacuum replaced hives of [handmade calculations](https://commons.wikimedia.org/wiki/File:Human_computers_-_Dryden.jpg#/media/File:Human_computers_-_Dryden.jpg)<sup>💡</sup>. Innovative mathematicians devised the realm and craft of **software** from the ground up and right before one's eyes.\
&nbsp;&nbsp;&nbsp;&nbsp;<sup>💡</sup> <sub>To be pedant: digital computers emerged in 1940s and already in 1950s they began shedding vacuum tubes for transistors.</sub>

The leap in hardware and programming could surpass the wildest sci-fi, and two letters could suggest that processors will soon be peers of human brains.

<details>
<summary><ins>&nbsp;These two letters were <b>not</b> <mark>AI</mark> but ...&nbsp;</ins></summary>
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;... **`IF`**.

> High-level languages (<samp>FORTRAN</samp>, <samp>ALGOL</samp>), described algorithms close to usual English, and the **IF** statement (along with `GOTO`) introduced the feeling of human doubt and decision-making.

\__________________________________________
</details>

IT prognoses and concerns, even among the most mathematician masterminds, went off the scale (akin to over-optimism in space exploration).<sup>🙋</sup> Much of today's hits are remakes from that generation, like GPTs rooted in generative literature of that time.

&nbsp;&nbsp;&nbsp;&nbsp;<sup>🙋</sup> <sub>However, not everyone was on this train, like [Hubert Dreyfus](../../quotes/README+/contributors/README.md#Hubert-Dreyfus).</sub>

### Late 1970s - to 1980s

<table><tr><td><a href="../../../../_rsc/_img/snap/media/1983.Androbot_Topo-Ad_FullPage.jpg"><img width="500px" alt="&nbsp;&nbsp;&nbsp;Androbot Topo, wireless to an Apple II computer, $1'195 with speech, 1983"
 src="../../../../_rsc/_img/snap/media/1983.Androbot_Topo-Ad.jpg" title="&nbsp;Androbot Topo, wireless to an Apple II computer, $1'195 with speech, 1983" />
</a></td><td>
<p>Computers got thinner and gradually spread from science and government tasks to business (with a much greater audience, spectrum of functions, and a bit of private time for amusement). Operating Systems grew into a separate phenomenon.</p>
<p>Robotics delivered funny androids. Programs could support a kind of console dialog for consulting or entertainment (where some confidently saw sparks of sentience on both ends).</p>
<p>In chess, with its intellectual aura, programs could give winning hints (not for cheating as nowadays but as experiments) that grandmasters couldn't discern and that was attributed to the abrupt progress of their inferior opponents.<sup>🎲</sup></p>
&nbsp;&nbsp;&nbsp;&nbsp;<sup>🎲</sup> <sub>It's AlphaGo in 2023 for another strategy game.</sub>
</td></tr></table>


### Late 1980s - early 1990s

PCs became a folk phenomenon at work and home. Thousands of enthusiasts, fascinated by the might and ease of programming, along with top engineers, which one could hire then for symbolic rates, tried to reanimate AI.

A side-effect of that kick was backyard pseudo-sciences of _inventive problem-solving_.

---

And here we are when _there will be no (human) programmers in five years&nbsp;_<sup>:one:</sup>. — No fear, thanks to AI _you'll have universal basic income_&nbsp;<sup>:two:</sup>, and _AI can now make you immortal_&nbsp;<sup>:three:</sup>, ...\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:one:</sup> <sub>*Emad Mostaque*, CEO of Stability AI, 2023</sub>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:two:</sup> <sub>_Prof. Geoffrey Hinton_ ("AI godfather") </sub>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:three:</sup> <sub>_Bernard Marr_, Forbes, Feb 21, 2023 </sub>

... and when the 2024<sup>the</sup> Nobel Prize in **Physics** goes to ... "_machine learning with artificial neural networks_".

### ⌛ Let's recap in 2030s ...

\______\
<sub>Images credit: Androbot, Wiki Commons, IMLO (learncomputerscienceonline.com), imdb.com. screengrabs of social media</sub>\
 🔚 &nbsp;🌘 kyriosity 2023-2024
