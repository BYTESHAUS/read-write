# Artificial Intelligence - 2023 - Much ado about ... fussing?

AI is on trend again, and one can't dodge this "IntelliPunk" festival - every other lecture, article, or product release names it.

Torrents of news and events have urged prominent software engineers, and tech leads [think of a dam](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) for [AI dystopia](https://www.businessinsider.com/ai-extinction-risk-openai-deepmind-anthropic-ceos-sam-altman-2023-5), while other visions are:

<details>
<summary><ins>&nbsp;Inspiring mainstream&nbsp;</ins></summary>
&nbsp;
 
* *Sundar Pichai*, Google CEO:\
"AI is the most profound technology humanity is working on today."
* *Jensen Huang*, CEO of NVIDIA:\
"Software is eating the world, but AI is going to eat software."
* MkCinsey & Company:\
"Generative AI’s impact on productivity could add trillions of dollars [annually]..."
* *Ray Kurzweil*, inventor and futurist:\
"By 2029, computers will have emotional intelligence and be convincing as people."
* _Gray Scott_, futurist, techno-philosopher, founder and CEO of SeriousWonder:\
  There is no reason and no way that a human mind can keep up with an artificial intelligence machine by 2035."
* _Sam Altman,_ OpenAI CEO (or not CEO, please prove):\
"AI will probably most likely lead to the end of the world, but in the meantime, there'll be great companies."
* [and how without him] *Elon Musk*, xAI startup founder:\
"The goal of xAI is to understand the true nature of the universe."

\__________________________________________
</details>

<details>
<summary><ins>&nbsp;Sparse pragmatism&nbsp;</ins></summary>

* *Ginni Rometty*, former CEO of IBM\
"Some people call this artificial intelligence, but the reality is this technology will enhance us. So instead of artificial intelligence, I think we'll augment our intelligence."

* _Christopher Nolan_, filmmaker, questioned about AI in a 2023 interview:\
"... the real world is, by definition, infinitely complex. ... And so, any digital simulation or technology that simulates, eventually, it always hits a particular limitation."

* _Michael Atleson_, Attorney, Federal Trade Commission, 27/Feb/2023:\
"Keep your AI claims in check."

\__________________________________________
</details>

<details>
<summary><ins>&nbsp;Philistine margins (as of mine)&nbsp;</ins></summary>
&nbsp;

* **AI Lab, ~1500s** (yes, AI)

![Faust vs. homunculus, engraving](../_rsc/_img/Homunculus.engraving.wiki.jpg)

* **Alchemy workshop, 2020s**

![Illustration of IT and AI](../_rsc/_img/ComputerScience-Intro(learncomputerscienceonline.com).jpg)

*&nbsp;<sub>Images are for illustrative purposes only and were taken from Wiki Commons and IMLO (learncomputerscienceonline.com)</sub>

\__________________________________________
</details>

## Stumbling block

Neither philosophy nor exact sciences nor neurobiology can formulate _intelligence_. Its genesis and individual rise are a bigger mystery. 
Digitalization speaks in clear definitions - and there are none.&nbsp;<sup>⚙️</sup>

Adepts of both bright and dismal AI future readily assume its *self-knowledge/awareness* as a trivial matter of time. 
To either brew by itself in increasingly complex code bouillon or get all of a sudden [short circuited](https://www.imdb.com/title/tt0091949)&nbsp;:cinema:.

Digital tech fantastically and reliably automates much of hard cogitative ops<sup>:expressionless:</sup>, but not unformalizable processes - abstraction, (self-)criticism, motivation - which are preconditions of great breakthroughs in exact sciences&nbsp;<sup>:art:</sup>.

&nbsp;&nbsp;&nbsp;&nbsp;<sup>⚙️</sup> <sub>Publicized views of decision-making as a relay work of neurons preset by acquired knowledge is an ingenuous simplism of free will debate.</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;<sup>:expressionless:</sup> <sub>Where humans are slow, prone to errors, lack parallelism, and finally get distracted, fatigued, or frustrated.</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;<sup>:art:</sup> <sub>The same in non-exact studies and art isn't that evident and requires a separate survey.</sub>

## Evidence

You can still think up a **:canada:**:parking::razor::switzerland::a:, which a diligent fellow will unweave but Internet-connected "AI" may only exhaust (unless you limit attempts).

The progress of computing powers allowed assisted mathematical proofs, impossible before. However, no AI with all the processing units in the world can solve an open mathematical problem&nbsp;<sup>:1234:</sup>. Humans do.&nbsp;<sup>:scientist:</sup>

A chess app on hand-held trash will smash any grandmaster but no AI will generate a better engine than hominid.&nbsp;<sup>♟️</sup>

IDEs and assists strongly boost dev productivity, but if any AI claims to write the code, it will be either a snippet or most rated Q&A or you need to describe the requirements closely to operators.

There's unadvertised human assistance and intervention behind many AI processes (either individual or by thousands in low-rate offshore). Recall the "machine learning" of [Mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk).

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:1234:</sup> <sub>Conditions for many are plain and unambivalent enough to submit them as a clause to a chatbot.</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:scientist:</sup> <sub>As _Grigori Perelman_ with one of the seven _Millenium Problems_ (announced in 2010).</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>♟️</sup> <sub>_Stockfish_ at the moment, but could be ancient IBM Deep Blue or even Deep Thought.</sub>

## Conclusion

**AI** now is a good old _deus ex machina_ as an idea, publicity of all kinds (much viral) in practice, and an overhype of IT potentials<sup>:hammer_and_wrench:</sup>. Compare to hash functions on pure math which had silently revolutionalized IT long before the blockchain boom.

The advanced math-phys and research behind "AI" (e.g. neural networks, quantum mechanics), and rising computing powers aren't magic but the hard work of natural intelligence.

&nbsp;&nbsp;&nbsp;&nbsp;<sup>:hammer_and_wrench:</sup> <sub>Processing global amounts of data in real-time, and finding patterns there; image/video on-the-fly recognition and filtering; routine translation and text generation; driver-assistance.</sub>

## Appendix. Preceding AI tides

Grunting "again"<sup>:top:</sup> obliges me to reminisce other AI booms, impromptu and without wiki. I skip old fiction (Timber Intelligence of Pinocchio and  a bit more scientific), the times when man humanized mechanical or electrical breakthroughs, and would resume on the digital era.

### 1950s - The crucial IT decade ever

Energized racks of metals, glass, and vacuum replaced hives of [handmade calculations](https://commons.wikimedia.org/wiki/File:Human_computers_-_Dryden.jpg#/media/File:Human_computers_-_Dryden.jpg). Creative mathematicians devised the realm and craft of **software** from the ground up and right before one's eyes. 

The leap in hardware and programming could surpass the wildest sci-fi, and two letters could suggest that processors will soon be peers of human brains.

<details>
<summary><ins>&nbsp;These two letters were not 'AI' but ...&nbsp;</ins></summary>
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;... **IF**.

> High-level languages, FORTRAN or COBOL, described algorithms close to usual English, and the **IF** statement introduced the feeling of human doubt and decision-making.

\__________________________________________ 
</details>

### Late 1970s

Computers got thinner and gradually spread from science calculations to business (with a much greater audience and spectrum of tasks). Operating Systems grew into a separate phenomenon. 

Robotics delivered funny androids. Programs could support a kind of console dialog for consulting or entertainment.

In chess, with its intellectual aura, programs could give winning hints (not for cheat as nowadays but an experiment) that grandmasters couldn't discern and attributed to the abrupt progress of their inferior opponents.&nbsp;<sup>:game_die:</sup>

&nbsp;&nbsp;&nbsp;&nbsp;<sup>:game_die:</sup> <sub>It's AlphaGo in 2023 for another strategy game, since a computer could defeat a chess WM in 1997.</sub>

### Late 1980s - early 1990s

PCs had become a folk factor at work and at home. Thousands of enthusiasts, fascinated by the might and ease of programming, along with top engineers, which one could hire then for symbolic rates, tried to reanimate AI.

A side-effect of that kick was the emergence of pseudo-sciences like _the theory of inventive problem-solving_.
___

And here we are in **2023**, when _there will be no (human) programmers in five years&nbsp;_<sup>:one:</sup>. No fear, thanks to AI _you'll have universal high income_&nbsp;<sup>:two:</sup>, and _AI can now make you immortal_&nbsp;<sup>:three:</sup>.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:one:</sup> <sub>*Emad Mostaque*, CEO of Stability AI, 2023</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:two:</sup> <sub>_Elon Musk_ again, Nov 3, 2023</sub>\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>:three:</sup> <sub>_Bernard Marr_, Forbes, Feb 21, 2023 </sub>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⌛ LET'S RECAP AFTER FIVE YEARS :microscope: ...
